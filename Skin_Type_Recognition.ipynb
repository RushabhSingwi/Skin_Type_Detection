{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kbge_fDCnNu_",
    "outputId": "8eb96687-1e2c-45eb-aec5-59af394e8523"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('helper_functions.py', <http.client.HTTPMessage at 0x10550eb30>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/Hrushi11/Dogs_VS_Cats/main/helper_functions.py'\n",
    "urllib.request.urlretrieve(url, 'helper_functions.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (70.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.1)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.2 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.1\n",
      "    Uninstalling tzdata-2024.1:\n",
      "      Successfully uninstalled tzdata-2024.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.2 requires numpy<2,>=1.19.3, but you have numpy 2.0.0 which is incompatible.\n",
      "streamlit 1.32.2 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n",
      "tensorflow 2.17.0 requires numpy<2.0.0,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
      "pyarrow 15.0.2 requires numpy<2,>=1.16.6, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.0.0 pandas-2.2.2 python-dateutil-2.9.0.post0 pytz-2024.1 six-1.16.0 tzdata-2024.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pillow\n",
      "  Using cached pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Using cached pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Installing collected packages: Pillow\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.2 requires numpy<2,>=1.19.3, but you have numpy 2.0.0 which is incompatible.\n",
      "streamlit 1.32.2 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Pillow-10.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.53.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (162 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting numpy>=1.23 (from matplotlib)\n",
      "  Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached matplotlib-3.9.1-cp312-cp312-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Using cached contourpy-1.2.1-cp312-cp312-macosx_11_0_arm64.whl (245 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.53.1-cp312-cp312-macosx_11_0_arm64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.5-cp312-cp312-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: six, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, python-dateutil, contourpy, matplotlib\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.53.1\n",
      "    Uninstalling fonttools-4.53.1:\n",
      "      Successfully uninstalled fonttools-4.53.1\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.12.1\n",
      "    Uninstalling cycler-0.12.1:\n",
      "      Successfully uninstalled cycler-0.12.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.2.1\n",
      "    Uninstalling contourpy-1.2.1:\n",
      "      Successfully uninstalled contourpy-1.2.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.9.1\n",
      "    Uninstalling matplotlib-3.9.1:\n",
      "      Successfully uninstalled matplotlib-3.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.2 requires numpy<2,>=1.19.3, but you have numpy 2.0.0 which is incompatible.\n",
      "streamlit 1.32.2 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n",
      "tensorflow 2.17.0 requires numpy<2.0.0,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
      "pyarrow 15.0.2 requires numpy<2,>=1.16.6, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.1 numpy-2.0.0 packaging-24.1 pillow-10.4.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 six-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall Pillow\n",
    "!pip install --force-reinstall matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.5.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: https://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/__check_build:\n__init__.py               __pycache__               meson.build\n_check_build.cpython-312-darwin.so_check_build.pyx\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/__check_build/__init__.py:49\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_check_build\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_build  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.__check_build._check_build'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/__init__.py:80\u001b[0m\n\u001b[1;32m     69\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of sklearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/__check_build/__init__.py:51\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_check_build\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_build  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mraise_build_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/__check_build/__init__.py:32\u001b[0m, in \u001b[0;36mraise_build_error\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m             dir_content\u001b[38;5;241m.\u001b[39mappend(filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"%s\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m___________________________________________________________________________\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mContents of %s:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m%s\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m___________________________________________________________________________\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mIt seems that scikit-learn has not been built correctly.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mIf you have installed scikit-learn from source, please do not forget\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mto build the package before using it: run `python setup.py install` or\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m`make` in the source directory.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m%s\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;241m%\u001b[39m (e, local_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(dir_content)\u001b[38;5;241m.\u001b[39mstrip(), msg)\n\u001b[1;32m     45\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/__check_build:\n__init__.py               __pycache__               meson.build\n_check_build.cpython-312-darwin.so_check_build.pyx\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Q54pHOqSX78"
   },
   "outputs": [],
   "source": [
    "# Importing dependancies\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from helper_functions import plot_loss_curves, compare_historys, make_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQwIjUBFS2td"
   },
   "outputs": [],
   "source": [
    "# Setting dir paths\n",
    "oil_dir = \"/Users/rushabh/Code/AI/Mirror_AI/skin-dataset/oily\"\n",
    "dry_dir = \"/Users/rushabh/Code/AI/Mirror_AI/skin-dataset/dry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_Q6CLHrVjEJ"
   },
   "outputs": [],
   "source": [
    "# Getting all the paths in a list for oil and dry skins\n",
    "oil_list = []\n",
    "for path in os.listdir(oil_dir):\n",
    "  oil_list.append(oil_dir + \"/\" + path)\n",
    "\n",
    "dry_list = []\n",
    "for path in os.listdir(dry_dir):\n",
    "  dry_list.append(dry_dir + \"/\" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGbCcYVHYa9u"
   },
   "outputs": [],
   "source": [
    "# Preprocess img function\n",
    "IMG_SIZE = (224, 224)\n",
    "def load_and_prep(filepath):\n",
    "  img_path = tf.io.read_file(filepath)\n",
    "  img = tf.io.decode_image(img_path)\n",
    "  img = tf.image.resize(img, IMG_SIZE)\n",
    "\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQGnzlea-ItV"
   },
   "outputs": [],
   "source": [
    "# Getting only those images which are suitable \n",
    "def corrImgs(arr):\n",
    "  corrImgList = []\n",
    "  for path in arr:\n",
    "    img = load_and_prep(path)\n",
    "    if (img.shape[2] == 3) | (img.shape == 4):\n",
    "      corrImgList.append(path)\n",
    "  \n",
    "  return corrImgList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "9_o6zVe2Vp60",
    "outputId": "9c0020bf-ea86-46bb-f92d-cfcbe0b57b47"
   },
   "outputs": [],
   "source": [
    "# Randomly plotting the Oily Skin images\n",
    "random_oil_skin = np.random.choice(oil_list, size=12, replace=False)\n",
    "plt.figure(figsize=(17, 12))\n",
    "for i in range(12):\n",
    "  img_path = random_oil_skin[i]\n",
    "  img = load_and_prep(img_path)\n",
    "\n",
    "  # plotting the images\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.axis(False)\n",
    "  plt.imshow(img/255)\n",
    "  plt.title(\"Oily Skin\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "mcbLc0JaWihD",
    "outputId": "ab06cd99-b509-43b2-9664-b6f3fe76b543"
   },
   "outputs": [],
   "source": [
    "# Randomly plotting the Dry Skin images\n",
    "random_dry_skin = np.random.choice(dry_list, size=12, replace=False)\n",
    "plt.figure(figsize=(17, 12))\n",
    "for i in range(12):\n",
    "  img_path = random_dry_skin[i]\n",
    "  img = load_and_prep(img_path)\n",
    "\n",
    "  # plotting the images\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.axis(False)\n",
    "  plt.imshow(img/255)\n",
    "  plt.title(\"Dry Skin\", color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-7HlfvbZooE"
   },
   "outputs": [],
   "source": [
    "# Creating data augmentation layer\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "   preprocessing.RandomFlip(\"horizontal\"),\n",
    "   preprocessing.RandomRotation(0.2),\n",
    "   preprocessing.RandomZoom(0.2),\n",
    "   preprocessing.RandomHeight(0.2),\n",
    "  preprocessing.RandomWidth(0.2)\n",
    "], name=\"data_augmentation_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "wG8qbKW0cn4I",
    "outputId": "e1d4ed09-a6cf-48fe-dd7b-56e76b4a74b1"
   },
   "outputs": [],
   "source": [
    "# Visualizing multiple randomly augmented oily skin images\n",
    "plt.figure(figsize=(17, 12))\n",
    "for i in range(1, 12, 2):\n",
    "  random_class = \"Oily Skin\"\n",
    "  random_img_path = random.choice(oil_list)\n",
    "\n",
    "  img = load_and_prep(random_img_path)\n",
    "  aug_img = data_augmentation(tf.expand_dims(img, axis=0))\n",
    "  aug_img = tf.image.resize(aug_img[0], IMG_SIZE)\n",
    "\n",
    "  # Plotting original image\n",
    "  plt.subplot(3, 4, i)\n",
    "  plt.axis(False)\n",
    "  plt.imshow(img / 255)\n",
    "  plt.title(f\"Original Image: \\n{random_class}\", color=\"green\")\n",
    "\n",
    "  # Plotting augmented image\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.axis(False)\n",
    "  plt.imshow(aug_img / 255)\n",
    "  plt.title(f\"Augmented Image: \\n{random_class}\", color=\"blue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "g7DMQ3jedRQg",
    "outputId": "c1e4b9ac-7379-498d-e79d-30b0ddfdef1b"
   },
   "outputs": [],
   "source": [
    "# Visualizing multiple randomly augmented dry skin images\n",
    "plt.figure(figsize=(17, 12))\n",
    "for i in range(1, 12, 2):\n",
    "  random_class = \"Dry Skin\"\n",
    "  random_img_path = random.choice(dry_list)\n",
    "\n",
    "  img = load_and_prep(random_img_path)\n",
    "  aug_img = data_augmentation(tf.expand_dims(img, axis=0))\n",
    "  aug_img = tf.image.resize(aug_img[0], IMG_SIZE)\n",
    "\n",
    "  # Plotting original image\n",
    "  plt.subplot(3, 4, i)\n",
    "  plt.axis(False)\n",
    "  plt.imshow(img / 255)\n",
    "  plt.title(f\"Original Image: \\n{random_class}\", color=\"green\")\n",
    "\n",
    "  # Plotting augmented image\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.axis(False)\n",
    "  plt.imshow(aug_img / 255)\n",
    "  plt.title(f\"Augmented Image: \\n{random_class}\", color=\"blue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7v4wfDDdfox",
    "outputId": "b6c46372-e88c-4c97-ea09-218345a6730f"
   },
   "outputs": [],
   "source": [
    "# Train test splits\n",
    "data_gen = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "train_data = data_gen.flow_from_directory(directory=\"/Users/rushabh/Code/AI/Mirror_AI/skin-dataset\",\n",
    "                                          subset='training',\n",
    "                                          target_size=IMG_SIZE)\n",
    "\n",
    "test_data = data_gen.flow_from_directory(directory=\"/Users/rushabh/Code/AI/Mirror_AI/skin-dataset\",\n",
    "                                         subset='validation',\n",
    "                                         target_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyXoWqEOflZo",
    "outputId": "f7582d7c-14c3-4511-b7b7-f523c082bd63"
   },
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4na_tIof3Bn"
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIfLHMmRfn0Q",
    "outputId": "f55f1ee0-157f-49c6-8596-03e7450a782c"
   },
   "outputs": [],
   "source": [
    "# Setting up base model\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Setting up input layer\n",
    "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "x = data_augmentation(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling\")(x) \n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"output_layer\")(x) \n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# compiling the model\n",
    "model_1.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model\n",
    "history_1 = model_1.fit(train_data,\n",
    "                        epochs=5,\n",
    "                        steps_per_epoch=len(train_data),\n",
    "                        validation_data=test_data,\n",
    "                        validation_steps=0.25 * len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qkz8oRUOfrZe",
    "outputId": "da65724b-5d93-4c17-92ae-f4483ef63e09"
   },
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model_1.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "5qcgnu3InIzm",
    "outputId": "9be95170-541b-4810-e451-961f65b0f8e3"
   },
   "outputs": [],
   "source": [
    "# Plotting loss curves\n",
    "plot_loss_curves(history_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eusxGLTwnYZg",
    "outputId": "d5bd0440-0587-4f5f-a612-b689f5e81b72"
   },
   "outputs": [],
   "source": [
    "# Saving the model \n",
    "model_1.save(\"/Users/rushabh/Code/AI/Mirror_AI/skin-dataset/RealTimeDetections.h5\", save_format=\"h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_QVY_93rUcm",
    "outputId": "ad8ff3b0-7802-4ca2-f137-94c986f2e3c1"
   },
   "outputs": [],
   "source": [
    "# Unfreeze all of the layers in the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Refreeze every layer except for the last 5\n",
    "for layer in base_model.layers[:-5]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Recompile model with lower learning rate\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune for 5 more epochs\n",
    "fine_tune_epochs = 10 \n",
    "\n",
    "history_1_fine_tune_1 = model_1.fit(train_data,\n",
    "                                    epochs=fine_tune_epochs,\n",
    "                                    validation_data=test_data,\n",
    "                                    validation_steps= 0.25 * len(test_data), \n",
    "                                    initial_epoch=history_1.epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "jqhcWsIwtUWj",
    "outputId": "139bf6c0-ccf3-44da-86f2-08a4729c664d"
   },
   "outputs": [],
   "source": [
    "compare_historys(history_1, history_1_fine_tune_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f3_1XvILcBY",
    "outputId": "25b795b6-7ba1-40cd-da23-ee656fe5b1e8"
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-uo6C3cCtmNM",
    "outputId": "8d2704d3-fa86-456b-ecf9-57aec4e40616"
   },
   "outputs": [],
   "source": [
    "# Getting test image file paths\n",
    "test_data_img_paths = test_data.filepaths\n",
    "np.random.shuffle(test_data_img_paths)\n",
    "\n",
    "# Getting labels\n",
    "test_data_labels = []\n",
    "for path in test_data_img_paths:\n",
    "  test_data_labels.append(path.split(\"/\")[5])\n",
    "\n",
    "# Creating a dataframe of test paths\n",
    "df_test_data_img_paths = pd.DataFrame({\"Img Paths\": test_data_img_paths,\n",
    "                                       \"Labels\": test_data_labels})\n",
    "\n",
    "df_test_data_img_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kXLLnS60ON2",
    "outputId": "a8d9041f-a65d-439a-ef12-bf8a43105588"
   },
   "outputs": [],
   "source": [
    "len(df_test_data_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9cChn1MxFDR",
    "outputId": "e312ff06-1598-4fbc-cdb8-1ce1e7a08874"
   },
   "outputs": [],
   "source": [
    "# Generating a test dataset for \n",
    "test_data_shuff = data_gen.flow_from_dataframe(df_test_data_img_paths, \n",
    "                                               directory=\"/Users/rushabh/Code/AI/Mirror_AI/skin-dataset\", \n",
    "                                               x_col='Img Paths', y_col='Labels',\n",
    "                                               shuffle=False,\n",
    "                                               target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7MPwIHHzUph",
    "outputId": "77af808a-a091-4ce5-f7d6-c3f34141e57f"
   },
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "# The same model architecture was trained on tensorflow==2.3.0 and then loaded here, trained on Tesla-K80\n",
    "model = tf.keras.models.load_model(\"/Users/rushabh/Code/AI/Mirror_AI/Skin-Type-Recognition-Degraded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjsFxTewyTio",
    "outputId": "80d2d61e-e314-480a-d381-adc245218a92"
   },
   "outputs": [],
   "source": [
    "# Getting prediction probabilites\n",
    "pred_prob = model.predict(test_data_stuff)\n",
    "pred_prob[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F456O2x8y819",
    "outputId": "3363b0a7-e609-4d46-dbc4-6baadcf52ae2"
   },
   "outputs": [],
   "source": [
    "y_pred = pred_prob.argmax(axis=1)\n",
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHG4lCCMy-iC",
    "outputId": "cceae041-6aeb-4940-c7b3-7ea15ca82557"
   },
   "outputs": [],
   "source": [
    "# Labels \n",
    "test_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHLo1juO0DYa",
    "outputId": "ccb3e29f-0926-40ff-c7ba-41778b3f65cb"
   },
   "outputs": [],
   "source": [
    "# Converting our text labels into numbers\n",
    "y_true = []\n",
    "for label in test_data_labels:\n",
    "  label_ = 0 if label == \"dry\" else 1\n",
    "  y_true.append(label_)\n",
    "\n",
    "y_true  = np.array(y_true)\n",
    "y_true[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "YPfupwxj1n9M",
    "outputId": "d6b2b198-566d-4ce9-d755-78188cf15ec6"
   },
   "outputs": [],
   "source": [
    "# Plotting a confusion matrix\n",
    "class_names = [\"dry\", \"oily\"]\n",
    "\n",
    "make_confusion_matrix(y_true=y_true,\n",
    "                      y_pred=y_pred,\n",
    "                      classes=class_names,\n",
    "                      figsize=(8, 8),\n",
    "                      text_size=30,\n",
    "                      norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RR-pBASw5659"
   },
   "outputs": [],
   "source": [
    "# To get suitable images\n",
    "corrImgList = corrImgs(test_data_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "BwGkXLqs563m",
    "outputId": "4bc7a630-890b-498d-f2e1-922ec94784fd"
   },
   "outputs": [],
   "source": [
    "path_list = np.random.choice(corrImgList, size=12, replace=False)\n",
    "plt.figure(figsize=(17, 12))\n",
    "for i in range(12):\n",
    "  img_path = path_list[i]\n",
    "  class_name = img_path.split(\"/\")[5]\n",
    "  img = load_and_prep(img_path)\n",
    "\n",
    "  pred_prob  = model.predict(tf.expand_dims(img, axis=0))\n",
    "  pred_class = class_names[pred_prob.argmax()]\n",
    "\n",
    "  # Plot the image(s)\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.imshow(img/255.)\n",
    "  title_color = \"g\" if class_name == pred_class else \"r\"\n",
    "  plt.title(f\"actual: {class_name}, pred: {pred_class}, \\nprob: {pred_prob.max():.2f}%\", c=title_color)\n",
    "  plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6Tl4qE2_5HW",
    "outputId": "99b67dec-dae5-4d19-a26b-a0f0041fda93"
   },
   "outputs": [],
   "source": [
    "# Getting test labels\n",
    "test_labels = []\n",
    "for path in test_data_img_paths:\n",
    "  test_labels.append(path.split(\"/\")[5])\n",
    "\n",
    "test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "ORASmtvm561h",
    "outputId": "602cf7b9-cfaa-4abe-88ec-bc385176c0ce"
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({\"filename\" : test_data_img_paths,\n",
    "                         \"label\": test_labels,\n",
    "                         \"True\": y_true,\n",
    "                         \"pred\": y_pred})\n",
    "\n",
    "df_pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XafswDbTATKl"
   },
   "outputs": [],
   "source": [
    "df_pred.to_csv(\"test-preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBv2OhqHBdxN",
    "outputId": "7eb74b1a-7983-4916-92ff-47d620b55071"
   },
   "outputs": [],
   "source": [
    "len(test_data_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxoBMCWaArSP",
    "outputId": "f1c472e7-3b66-4cc5-f1f3-1af227bda432"
   },
   "outputs": [],
   "source": [
    "# Wrong predicted images\n",
    "wrng_pred_test_paths = []\n",
    "for i in range(len(test_data_img_paths)):\n",
    "  if y_true[i] != y_pred[i]:\n",
    "    wrng_pred_test_paths.append(test_data_img_paths[i])\n",
    "\n",
    "len(wrng_pred_test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "vNo5t8uMCAnG",
    "outputId": "76f3c73d-c08c-4f84-beb5-efd9b5bc01c8"
   },
   "outputs": [],
   "source": [
    "# Wrongly predicted images\n",
    "plt.figure(figsize=(17, 12))\n",
    "for i in range(4):\n",
    "  img_path = wrng_pred_test_paths[i]\n",
    "  class_name = img_path.split(\"/\")[5]\n",
    "  img = load_and_prep(img_path)\n",
    "\n",
    "  pred_prob  = model.predict(tf.expand_dims(img, axis=0))\n",
    "  pred_class = class_names[pred_prob.argmax()]\n",
    "\n",
    "  # Plot the image(s)\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.imshow(img/255.)\n",
    "  title_color = \"g\" if class_name == pred_class else \"r\"\n",
    "  plt.title(f\"actual: {class_name}, pred: {pred_class}, \\nprob: {pred_prob.max():.2f}%\", c=title_color)\n",
    "  plt.axis(False);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Skin_Type_Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
